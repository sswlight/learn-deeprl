## 相比A2C的优化
- 最大熵强化学习Maximum Entropy RL，增大运动的多样性
- 双价值网络
- 像TD3一样价值网络为DQN,根据s和a打分
- 策略网络输出实际为高斯分布的参数，实际输出的连续动作为tanh(均值+随机数*方差)