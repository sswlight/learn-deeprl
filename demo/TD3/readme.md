## 如何完成连续动作（DDPG）
- 价值函数：不再采用状态价值函数$V(s)$，而是采用DQN的方式训练当前状态与动作的最优价值$Q$
- 策略函数：相比正常A2C，将策略网络返回的动作及它的概率，最终随机选择一个动作，即$\pi$函数所得结果，转换为策略网络仅返回给定维度的一些数，每个数代表一个维度执行的连续动作，由于执行的动作是确定的，被称为DDPG
- 策略网络更新时，loss为负Q值
- 拥有经验回放，可以反复训练
## 相比DDPG的改进
- 双价值网络，取价值较低者更新
- 有targetnetwork
- 策略网络相比价值网络延迟更新，价值网络每更新10次，策略网络更新1次
- 平滑目标策略，给动作加噪音来探索未知